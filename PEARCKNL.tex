\graphicspath{{plots/}}
\section{Introduction}
% TODO: Move sections 1 and 2 of white paper into introduction

The Stampede 2 supercomputer at the Texas Advanced Computing Center (TACC) features the
latest Intel Xeon Phi processors, codenamed Knight's Landing (KNL). Th Indiana University
(IU) has  The increasing demand for high-performance cluster computing in data analysis,
driven by increasing data set size and the computational complexity of analyzing large
amounts of data, has rivaled that of scientific computing in recent years
\cite{fox:bdBenchmarking, kouzes:paradigm}.  The large amounts of RAM per node, high
storage capacity, and advanced processing capabilities of modern high-performance
computing (HPC) clusters are an attractive option for data analysts to perform their
computation given the large size of their data and the increasing emphasis on
computationally demanding analytical tools like machine learning [3, 4, 5].  Depending on
the nature of the analysis to be performed, analytics workflows may be implemented as
concurrent processes on several nodes of a computing cluster with little or no
coordination between them, or as highly coordinated parallel processes in which the nodes
perform portions of the same computational task. Examples of the former type of analysis
include Monte Carlo simulations and optimization problems that are solved for many
different initial conditions to compute an optimal result.  An example of the latter type
of analysis is the solution of large, sparse linear systems of such high order that the
computation of the solution can be effectively parallelized across the nodes. Regardless
of whether the workflow is implemented as concurrent or parallel processes, it is
important for data analysts to have software environments at their disposal which can
exploit the performance advantages of modern HPC clusters.  Data analysts have come to
rely heavily on the open source R programming environment to manipulate and analyze their
data.  The R programming environment provides a language, the R programming language,
which was designed around statistical computing, and an interpreter for executing R
programs [6].  The R language interpreter is implemented in the C programming language,
and the functionality of the programming environment can be extended through packages that
are commercially available or available as open source.  The ability to develop R packages
in C means that developers can extend the functionality of the environment and achieve
high performance.  This also gives the R interpreter the ability to link to external
libraries optimized for performance on a given system.  As an example, the R environment
can be compiled to dynamically link with optimized instances of the BLAS [7] and LAPACK
[8] libraries for performing linear algebra computations.  Optimization of the R
interpreter, its intrinsic functionality, and R packages for specific hardware
architectures will be necessary for data analysts to take full advantage of the latest HPC
clusters, and to obviate the need to reengineer their analysis workflows in another
language. Performance benchmarks provide a way to assess the performance of software on a
given computing platform and to compare performance of software across different
platforms.  The performance benchmark results can also be used to prioritize software
performance optimization efforts on emerging HPC systems.  One such system is the new
Stampede 2 supercomputer cluster being installed at the Texas Advanced Computing Center
(TACC).  The CPU used in the Stampede 2 system is the Intel Xeon Phi which is a many-core,
vector processor with 68 cores and two 512-bit vector processing units per core, a
sufficient deviation from the standard Xeon processors and Xeon Phi accelerators of the
original Stampede system to necessitate a performance assessment of the R programming
environment on both systems [9].  We developed an R performance benchmark to determine the
single-node run-time performance of compute intensive linear algebra kernels that are
common to many data analytics algorithms, and the run-time performance of machine learning
functionality commonly implemented with linear algebra operations.  We then performed
single-node strong scaling tests of the benchmark on both Stampede systems to determine
problem sizes and numbers of threads for which the Stampede 2 nodes were competitive or
outperformed their predecessors.  It is our intention that the results be used to guide
future performance optimization efforts of the R programming environment to increase the
applicability of HPC clusters like the Stampede 2 system to compute-intensive data
analysis. I.	The Need for a More Robust R Benchmark There are very few R performance
benchmarks, and there is no R performance benchmark that has been peer reviewed and
accepted as a standard.  The most common and robust benchmark is the R Benchmark [10]
which runs fifteen different microbenchmarks covering matrix formation, matrix
factorization, solving linear systems of equations, and sorting.  Other benchmarks like
bench [10], for example, are microbenchmarks that focus on a few specific computational
kernels in R, but they lack the coverage needed to comprehensively assess R performance in
a high-performance computing environment. Although R Benchmark is the most robust
benchmark publicly available, it lacks flexibility and it is focused only on testing
intrinsic functionality while leaving the performance of packages included in the standard
R distribution unassessed.  The benchmark is contained in a single R function which times
each R operation or function to be performance tested, but there is no functionality to
choose a specific operation or function to performance test. Furthermore, the size of the
input data to each test is hardcoded and there is no way to loop over larger problem sizes
as would be needed in scalability studies to test the effectiveness of parallelism.  Also,
users of the benchmark cannot set the number of runs over which to average the run time of
a specific function without changing the number of runs for all functions tested.  The
monolithic structure of R Benchmark also prevents configuring the microbenchmarks to run
concurrently in a cluster environment.


\section{Methodology and Approach}
% TODO: Methodology/approach section after introduction
  % Pull in from white paper section 3:
  %   1. focus on dense matrix, not sparse
  %   2. Broad discussion of matrix sizes, thread counts, strong scaling
  %   3. emphasize what we were getting at: SNB vs. KNL performance
  %      for similar thread counts and scalability
% JRM - This paragraph might belong in the introduction
Our performance testing objectives were to determine the following: which
  functions implemented multithreaded parallelism; the single-node
  run time and strong scaling on both the Sandy Bridge and Knights
  Landing nodes; and an estimate of the overhead incurred by R kernel functions
  when compared to similar kernel functionality executed exclusively in C.
Given that the kernel functions are used as components in many machine learning
  and analytics algorithms, it was important for us to determine which kernel
  functions performed poorly on the Xeon Phi architecture compared to their
  performance on the Xeon Sandy Bridge architecture.
Furthermore, it was worthwhile for us to run performance tests with
  similar kernel functionality exclusively in C to determine the potential
  for performance improvement of the wrapper functions which implement the
  kernels.
In addition to performance testing the kernel functions, we tested machine
  learning package functions to determine if those functions leverage any
  parallelized kernels.

To accomplish our objectives, we developed a robust R benchmark for HPC
  environments and used it to compare the performance of the new Xeon Phi-based
  nodes of the Stampede 2 system with the standard Xeon-based nodes of the
  original Stampede system.
The HPC benchmark performs microbenchmarking of compute-intensive dense linear
  algebra kernels and packages that can utilize optimized linear algebra
  kernels.
We executed each microbenchmark with a wide range of problem dimensions and
  thread counts to determine run time performance and scalability of the
  underlying R functionality on each type of node, using strong scaling as the
  metric to determine scalability for various thread counts.
% TODO: The sentence below indicates a goal not explicitly stated in the
% white paper; we should find a place for it, but it doen't fit under
% methodology.
%It is our intention that the HPC benchmark also be used in performance
%  optimization efforts.
\subsection{A New R Benchmark for HPC Environments}
We chose dense linear algebra kernels for inclusion in the benchmark
  because they form the computational foundation of many numerical methods
  frequently utilized in R applications.
The dense linear algebra kernels we included in the benchmark are: Cholesky
  factorization, eigendecomposition, linear least squares fit, dense linear
  solve, matrix cross product, matrix determinant, matrix-matrix multiplication,
  matrix-vector multiplication, QR factorization, and singular value
  decomposition.
%Many of these kernels are in \textit{R Benchmark}.
% JRM - Omit the descriptions of the kernels and their applicability until
% we know they won't push the paper length over the page limit
%The Cholesky factorization, which factors a symmetric positive definite matrix
% into two triangular factors, is often used to solve linear systems and to
% generate random variables from multivariate normal
% distributions~\cite{golub:matrix3}.
%The eigendecomposition decomposes a matrix into a matrix of eigenvectors and a
%  matrix of eigenvalues which have direct applications in science, engineering,
%  and data analysis~\cite{strang:linAlg93, strang:cse07, saad:largeEigen11}.
%The linear least squares fit is a ubiquitous method in modeling and statistics
%  for fitting a function with linear coefficients to a data
%  set~\cite{heath:scientific}.
%% TODO: This next sentence needs a reference
%The solution of a dense linear system of equations with multiple right-hand
%  sides is a common problem in modeling and simulation when multiple solutions
%  must be computed given a linear model of a system (the matrix) and several
%  measurements (the multiple right-hand sides).
%The fundamental linear algebra operations matrix cross product, matrix-matrix
%  multiplication, and matrix-vector multiplication are integral to other
%  kernels.
%The matrix determinant has practical applications in statistics as it is
% involved in the definition of various probability distributions~\cite{grimmett:probability}.
%The QR factorization, which factors a matrix into an orthogonal matrix and an
%  upper triangular matrix, is used in the solution of linear systems and as a
%  component of other decompositions such as the eigendecomposition~\cite{golub:matrix3, trefethen:numLinAlg}.
%The singular value decomposition, which decomposes a matrix into two unitary
%  matrices and a diagonal matrix, is a powerful decomposition with many
%  applications in data mining and machine learning~\cite{golub:matrix3, murphy:ml}.
We also included microbenchmarks of neural network training and cluster
  identification functions from the \textit{nnet} and \textit{cluster} packages,
  respectively, that are included in the standard R distribution.
The \texttt{nnet} function is used to train neural networks with a single hidden
  layer~\cite{ripley:pattern96}, and the \texttt{pam} function, which implements
  the partitioning around medoids algorithm~\cite{chu:kmedoids, reynolds:clustering},
  is used to perform the cluster identification.
We included benchmark tests of these packages because neural network training
  and cluster identification can be implemented with computational kernels
  tested in the benchmark.
The neural network training benchmark trains a neural network on a set of
  feature vectors drawn from a multivariate normal distribution to approximate
  the probability density function from which the vectors were drawn.
We chose to approximate a multivariate normal probability density function
  because the training problem is easy to scale in both number of training
  vectors and number of features.
The clustering benchmark generates normally distributed clusters in an
  $N$-dimensional, real-valued feature space where the mean of one cluster is
  located at the origin and the means of two clusters are each located at
  positions $-1$ and $1$ of each axis.

We structured the HPC benchmark so that each kernel or package function is
  tested within its own microbenchmark function.
Table~\ref{tab:microbenchmarks} shows the kernels and package functions tested
  by each microbenchmark.
Each microbenchmark can be configured to compute run-time performance for
  several problem sizes, where the run time with respect to each problem size
  is computed as an average over a configurable number of runs -- enabling
  users to test small problem sizes over a large number of runs to account
  for operating system interference.
Another advantage of implementing the microbenchmarks as separate R functions
  is that they can be executed concurrently in cluster environments.

\begin{table*}
  \caption{HPC Microbenchmarks and Corresponding Kernel or Package Functions Tested}
  \label{tab:microbenchmarks}
  \begin{tabular}{ll}
    \toprule
    Microbenchmark & Kernel/Package function \\
    \midrule
    Cholesky factorization       & \texttt{chol} \\
    eigendecomposition           & \texttt{eigen} \\
    linear least squares fit     & \texttt{lsfit} \\
    dense linear solve           & \texttt{solve} \\
    matrix cross product         & \texttt{crossprod} \\
    matrix determinant           & \texttt{determinant} \\
    matrix-matrix multiplication & $\%$$*$$\%$ \\
    matrix-vector multiplication & $\%$$*$$\%$ \\
    QR decomposition             & \texttt{qr} \\
    singular value decomposition & \texttt{svd} \\
    neural network training      & \texttt{nnet} \\
    cluster identification       & \texttt{pam} \\
    \bottomrule
  \end{tabular}
\end{table*}

% JRM - Moved to first paragraph in Methodology section
%\subsection{Performance Testing Objectives}
%Our performance testing objectives were to determine the following: which
%  functions implemented multithreaded parallelism; the single-node
%  run time and strong scaling on both the Sandy Bridge and Knights
%  Landing nodes; and an estimate of the overhead incurred by R kernel functions
%  when compared to similar kernel functionality executed exclusively in C.
%Given that the kernel functions are used as components in many machine learning
%  and analytics algorithms, it was important for us to determine which kernel
%  functions performed poorly on the Xeon Phi architecture compared to their
%  performance on the Xeon Sandy Bridge architecture.
%Furthermore, it was worthwhile for us to run performance tests with
%  similar kernel functionality exclusively in C to determine the potential
%  for performance improvement of the wrapper functions which implement the
%  kernels.
%In addition to performance testing the kernel functions, we tested the machine
%  learning package functions to determine if those functions leverage any
%  parallelized kernels.

\subsection{Description of Tested Systems}
We conducted single-node performance tests on the Sandy Bridge and Knights
  Landing compute nodes of the Stampede computing cluster.
Each Sandy Bridge compute node is a Dell PowerEdge C8220z with two eight-core,
  $2.7$~GHz ($3.5$~GHz turbo) Intel Xeon E5-2680 (Sandy Bridge architecture)
  processors with $20$~MB of L2 cache each; $32$~GB of $1600$~MHz DDR3 RAM; and
  a 61-core Intel Xeon Phi SE10P coprocessor.
Each Knights Landing compute node is a Dell PowerEdge with a single 68-core,
  $1.4$~GHz ($1.6$~GHz turbo) Intel Xeon Phi 7250 CPU (Knights Landing
  architecture); $96$~GB of $2400$~MHz DDR4 RAM;
  and $16$~GB of on-package Multi-Channel Dynamic Random Access Memory (MCDRAM)
  with $400$~GB/s bandwidth.
Each of the Xeon Phi cores has two 512-bit vector processing units which
  support a fused multiply-add operation.
The cores, which are grouped by tiles containing two cores and a shared
  $1$~MB L2 cache~\cite{intel:xeonphi}, communicate with each other via a
  two-dimensional mesh network to maintain cache consistency.

The Xeon Phi CPU can be configured in various clustering modes to group the
  tiles into quadrants or hemispheres to take advantage of data locality in
  applications~\cite{vladimirov:knlModes}.
The MCDRAM is high bandwidth memory that resides on the CPU package and can
  operate in different modes which are determined at boot
  time~\cite{vladimirov:knlModes, asai:mcdramKnl}.
When the CPU is in flat mode, applications can either explicitly allocate memory
  in MCDRAM, or they can be run from MCDRAM using the command line utility
  \texttt{numactl} to launch the application.
Programs executed with \texttt{numactl} allocate as much memory from
  MCDRAM as is available; any additional memory needed by the program is
  allocated in main memory.
When the CPU is in cache mode, the MCDRAM acts as a level 3 cache, operating
  transparently to applications; we tested all of the microbenchmarks in
  cache mode and a few in flat mode to compare performance in each mode.

We conducted the performance tests with the R programming environment version
  3.2.1 which we downloaded from the Comprehensive R Archive Network (CRAN).
On the Sandy Bridge nodes, which ran CentOS 6 (kernel
  2.6.32-431.17.1.el6.x86\_64), we built the programming environment with
  the Intel XE compiler versions 15.0 Update 2 (v. 15.0.2.164) and
  17.0 Update 1 (v. 17.0.1.132), and we linked the resulting executables with
  the parallel Intel Math Kernel Library (MKL) versions 11.2.2 and 2017 Update
  1, respectively.
On the Knights Landing nodes, which ran CentOS 7 (kernel
  3.10.0-327.36.3.el7.xpps1\_1.4.1.3272.x86\_64), we built the programming
  environment with the Intel XE compiler and linked with parallel MKL
  version 17.0 Update 1.
The \texttt{-xHost} option was applied to both Sandy Bridge builds, and the
  \texttt{-xMIC\_AVX512} option was applied to the Knights Landing build to
  utilize the vector instruction set.

%The compiler and MKL versions we used to build R on each system are given in
%  Table IV.
%Table V shows the compiling and linking options we used to build the R
%  distribution for each type of compute node for we tested performance with.
%The \texttt{xHost} option selects the highest instruction set available on the
%  Sandy Bridge processor.
%The \texttt{xMIC-AVX512} option is needed to select the AVX-512 vector
%  instruction set, the highest instruction set on Knights Landing, as the
%  \texttt{xHost} option is not supported for Xeon Phi processors.
%We passed the following options to GNU Autoconf to generate shared R libraries
%  and to link R with the MKL BLAS and LAPACK libraries: \texttt{--without-x},
%  \texttt{--enable-R-shlib}, \texttt{--enable-shared}, \texttt{--with-blas},
%  and \texttt{--with-lapack}.

% JRM - There is much more than results in this section.  Should it be
% titled Experiments and Results, or should we move it into Methodology
\section{Results}
% Part c of white paper section 4 to be moved into results section
%  Keep matrix dimension and experiment parameter specifics
%  in the results section (don't want this detail in Methodology and Approach)
%
We configured multithreading of each microbenchmark through OpenMP and MKL
  environment variables.
As recommended in the MKL developer's guide~\cite{intel:mkl2017} we set
  \texttt{KMP\_AFFINITY}=$granularity=fine,compact,1,0$.
We also disabled the dynamic thread adjustment controlled by
  \texttt{OMP\_DYNAMIC} and \texttt{KMP\_DYNAMIC} so that the scaling tests
  could be conducted
  ~\cite{intel:cpp2015, intel:cpp2017, intel:mkl11_2, intel:mkl2017}.
We tested scalability of the microbenchmarks by increasing the number of MKL
  threads through the environment variable \texttt{MKL\_NUM\_THREADS}.

The performance tests on the Sandy Bridge nodes included the two Xeon Sandy
  Bridge processors, but did not include any co-processing by the Xeon Phi
  coprocessor. 
Hyperthreading was not enabled on the Sandy Bridge processors, but it was
  enabled on the Knights Landing processors.
  coprocessor.
% JRM - This sentence ought to be adjacent to the rest of the matrix size
% descriptions 5 sentences down.
Small matrix dimensions of size $2000$ and $4000$ were often sufficient to
  determine performance cross over points for the kernel functions where one
  node type would begin to outperform the other given an increase in matrix
  size.
Larger matrices were useful for determining which kernels efficiently utilized a
  large number of threads on the Knights Landing nodes.
We observed no improvements in scalability by placing more than one thread on
  each core of the Knights Landing processors, but have included results for
  more than 68 threads for completeness.
The numbers of threads we tested on the Sandy Bridge nodes were:
  $1$, $2$, $4$, $8$, $12$, and $16$; and the numbers of threads we tested on
  the Knights Landing nodes were:
  $1$, $2$, $4$, $8$, $16$, $34$, $66$, $68$, $136$, $204$, and $272$.
Small matrix dimensions of size $2000$ and $4000$ were often sufficient to
  determine performance cross over points for the matrix kernel functions where one
  node type would begin to outperform the other given an increase in matrix
  size.
Larger matrices were useful for determining which kernels efficiently utilized a
  large number of threads on the Knights Landing nodes.
Apart from some exceptions, we tested the matrix kernels with
  square matrices having the following row dimensions $N$:
  $1000$, $2000$, $4000$, $8000$, $10000$, $15000$, $20000$, and $40000$; in the
  case of the least squares fit kernel, the matrix dimensions were
  $2N \times N/2$ for the same values of $N$.
For each combination of matrix dimension and number of threads for a given
  microbenchmark, we collected the run time and computed the strong scaling as
  the ratio of the run time of the microbenchmark to the run time of the
  microbenchmark with one thread.
We attempted to run performance tests up to matrix dimensions of $30000$ and
  $40000$ on the Sandy Bridge and Knights Landing nodes, respectively; however,
  some kernels could not be tested with these dimensions due to the memory
  capacity and internal size limitations imposed by R.
% JRM - Do we want to reference the spreadsheet?
%The test results for each microbenchmark, matrix size, and number of threads are
%  given in a spreadsheet accompanying this document.
We included plots showing microbenchmark run-time and strong scaling performance
  on Sandy Bridge and Knights Landing; we plotted results for
  matrix dimensions that reveal when the two node types become competitive for
  a microbenchmark, or to highlight when a node type consistently outperforms
  its competitor.
% JRM - Change S1 to SNB and S2 to KNL in remainder of text and in plots
In the plot legend entries, \textit{S1} indicates Sandy Bridge node performance
  and \textit{S2} indicates Knights Landing node performance; entries postfixed
  with a 15 or a 17 indicate tests were performed with the Intel 15 Update 2 or
  Intel 17 Update 1 compiler and MKL bundles, respectively.
% JRM - This sentence is out of place move toward beginning of section
In addition to the matrix kernels, we performance tested clustering and
  neural network training functionality from the \textit{nnet} and
  \textit{cluster} packages.

% JRM - This paragraph might belong in methodology
After we executed a given microbenchmark for a matrix dimension and all numbers
  of threads to be tested on a given node type, we determined the minimum run
  time over all tested numbers of threads.
We computed the minimum run times in this manner for each combination of node
  type, microbenchmark, and matrix dimension.
From these minimum values, we determined the fastest node type for each
  microbenchmark and matrix dimension so that we could quantify relative
  performance as the ratio of the minimum run time achieved by the slowest node
  to that achieved by the fastest node.
In the microbenchmark performance assessments that follow, the assessments are
  grouped by microbenchmarks that exhibited similar performance characteristics,
  and we provide relative performance ratios over the tested matrix dimensions.
We also provide the percentage of the maximum possible speedup, which we define
  as the speedup divided by the number of cores, achieved by each microbenchmark
  with the largest matrix tested.
Unless stated otherwise, the Sandy Bridge performance assessments are based on
  microbenchmarks built under the Intel 15 Update 2 bundle, and the Knights
  Landing performance assessments are based on microbenchmarks built under the
  Intel 17 Update 1 bundle.

The Sandy Bridge nodes consistently outperformed the Knights Landing nodes when
  running the Cholesky factorization (Figure 1), dense linear solve (Figure 5),
  and matrix determinant (Figure 7) kernels for the small matrix dimensions
  tested and with sixteen or fewer threads.
Comparing the best run times achieved by each node type, the Sandy Bridge nodes
  executed the Cholesky factorization up to $2.00$ times faster for tested
  matrix dimensions of $2000$ or less, the dense linear solve $1.77$ times
  faster for matrix dimensions of $1000$, and the matrix determinant $1.80$
  times faster for matrix dimensions of $1000$.
The Knights Landing nodes began to outperform the Sandy Bridge nodes for matrix
  dimensions at or above $2000$ or $4000$ for the same kernels, achieving run
  times that were $4.03$ (Cholesky factorization), $4.41$ (dense linear solve),
  and $5.23$ (matrix determinant) times faster than those achieved on the Sandy
  Bridge nodes given matrix dimensions of $30000$, $20000$, and $40000$,
  respectively.
The Sandy Bridge nodes achieved at most $65\%$, $81\%$, and $84\%$ of the
  maximum possible speedup with sixteen or fewer threads executing the Cholesky
  factorization kernel with tested matrix dimensions of $30000$ or less, the
  dense linear solve kernel with tested matrix dimensions of $20000$ or less,
  and the matrix determinant kernel with tested matrix dimensions of $40000$ or
  less, respectively.
The Knights Landing nodes achieved at most $38\%$, $48\%$, and $65\%$ of the
  maximum possible speedup for the same kernels and matrix dimensions.

The eigendecomposition kernel (Figure 2) performed up to $2.75$ times faster on
  the Sandy Bridge nodes than on the Knights Landing nodes for tested matrix
  dimensions of $8000$ or less.
As we increased the matrix dimension beyond $8000$, the Knights Landing nodes
  were up to $1.54$ times faster.
It is also interesting to note that the eigendecomposition kernel never scaled
  beyond eight threads on the Sandy Bridge nodes, regardless of matrix
  dimension, but the performance did not deteriorate as the maximum number of
  threads (sixteen) was reached either.
For tested matrix dimensions of $20000$ or less, the Knights Landing nodes
  achieved at most $5\%$ of the maximum possible speedup; the Sandy Bridge
  nodes achieved at most $17\%$ of the maximum possible speedup for the same
  matrix dimensions.

The least squares fit kernel (Figure 3) experienced difficulties scaling beyond
  four threads on the Sandy Bridge nodes.
Recall that the matrix dimensions in the case of the least squares fit are
  parametrized by $N$ such that the matrix is $2N \times N/2$.
The Sandy Bridge run times were up to $2.18$ times faster than those of the
  Knights Landing nodes for tested values of $N=8000$ or less, and the Sandy
  Bridge nodes achieved at most $10\%$ of the maximum possible speedup for
  tested values of $N=20000$ or less.
The Knights Landing run times were up to $1.46$ times faster than the Sandy
  Bridge nodes for tested matrix dimensions greater than $8000$.
The strong scaling on Knights Landing stagnated beyond eight threads, achieving
  only $5\%$ of the maximum possible speedup for tested values of $N=20000$ or
  less.
Based on this analysis and the stagnant or degraded strong scaling shown in
  Figure 3, we concluded that the linear least squares fit kernel is not
  scalable beyond a few threads on either node type.

The Knights Landing nodes were consistently competitive with or outperformed the
  Sandy Bridge nodes for all matrix dimensions and numbers of threads tested
  with the matrix cross product (Figure 6) and matrix-matrix multiplication
  (Figure 8) kernels.
The matrix cross product kernel was up to $4.27$ times as fast on the Knights
  Landing nodes than on the Sandy Bridge nodes for tested matrix dimensions of
  $40000$ or less.
The matrix-matrix multiplication kernel was up to $3.12$ times as fast on the
  Knights Landing nodes than on the Sandy Bridge nodes for tested matrix
  dimensions greater than $2000$; for tested matrix dimension of $2000$ or less,
  the best Sandy Bridge nodes were up to $2.11$ times faster than the Knights
  Landing nodes.
The Knights Landing and Sandy Bridge nodes executing the matrix cross product
  kernel achieved $51\%$ and $84\%$ of their maximum possible speedups,
  respectively.
The Knights Landing and Sandy Bridge nodes executing the matrix-matrix
  multiplication kernel achieved $43\%$ and $96\%$ of their maximum possible
  speedups, respectively.

The matrix-vector multiplication kernel on the Sandy Bridge nodes consistently
  outperformed the Knights Landing nodes for all tested matrix dimensions and
  numbers of threads (Figure 9).
The Sandy Bridge nodes ran the kernel up to $6.14$ times faster than the
  Knights Landing nodes for matrix dimensions of $40000$ or less.
The speedups achieved by the Knights Landing and Sandy Bridge nodes with the
  kernel were at most $2\%$ and $9\%$, respectively.
The poor scalability may be because the single-core performance is very high,
  and matrices of larger dimension could not be tested to increase the workload
  per core because of the matrix dimension limitations imposed by R.

We tested the QR decomposition kernel using both the LINPACK and LAPACK
  routines.
The QR kernel executed with the LAPACK implementation performed the best on both
  nodes, the results for which are given in Figure 10.
The kernel performed up to $1.48$ times faster on the Sandy Bridge nodes for
  matrix dimensions of $2000$ or less.
For matrix dimensions of $4000$ or more, the Knights Landing nodes were up to
  $6.32$ times faster than the Sandy Bridge nodes.
The Knights Landing nodes achieved speedups of at most $28\%$ of the maximum
  possible, and the Sandy Bridge nodes achieved speedups of at most $38\%$ of
  the maximum possible.

The singular value decomposition kernel exhibited similar performance
  (Figure 11).
For each tested matrix dimension of $2000$ or less, the Sandy Bridge nodes were
  up to $1.78$ times faster than the Knights Landing nodes.
For tested matrix dimensions of $4000$ or greater, the Knights Landing nodes
  were up to $4.25$ times as fast as those achieved on the Sandy Bridge nodes.
The percentages of the maximum possible speedup achieved by the Knights Landing
  and Sandy Bridge nodes were at most $23\%$ and $22\%$, respectively.

We also tested the performance of the Cholesky factorization, dense linear
  solve, and matrix cross product kernels when run exclusively out of the
  high-bandwidth MCDRAM.
We accomplished this by executing the kernels on Knights Landing nodes booted in
  flat mode and used the \texttt{numactl} utility with the \texttt{--preferred} option
  set to indicate that as much memory as possible for the R programming
  environment was to be allocated in the MCDRAM NUMA node; the environment and
  the dynamically allocated matrices we tested did fit entirely in MCDRAM.
The performance results, plotted in Figure 12, show no appreciable differences
  in kernel performance when the MCDRAM is configured in cache mode or flat
  mode.

We conducted additional experiments with the Sandy Bridge nodes to determine if
  the least squares fit, dense linear solve, or QR decomposition microbenchmarks
  would show improvement under the Intel 17 Update 1 compiler and MKL.
The dense linear solve and QR decomposition microbenchmark results showed little
  or no improvement over their Intel 15 Update 2 counterparts; however, the
  least squares fit kernel exhibited improved scalability beyond four threads
  for moderate to large matrix dimensions (Figure 4).

We performance tested the \texttt{nnet} function from the \textit{nnet} package
  and the \texttt{pam} function of the \textit{cluster} package to determine if
  those commonly used capabilities were parallelized.
We chose the neural network training parameters listed in Table~\ref{tab:nnetParams} to
  performance test the \texttt{nnet} function because they required the
  underlying solver to perform several iterations to achieve the required
  convergence.
Our tests revealed that the neither of the package functions parallelized, as
  was indicated by the run times and analysis tools used to determine if
  multiple threads were being created to perform the computation.
Nevertheless, the tests are useful for assessing performance of commonly used R
  libraries on each node type.
The Sandy Bridge nodes were approximately $3.5$ times faster and $3.0$ times
  faster than the Knights Landing nodes at training the neural network with
  three features and five features (Table~\ref{tab:nnetResults}), respectively.
Note that the run times decreased when we increased the number of feature
  vectors from $5000$ to $10000$ and the feature vector was of dimension three.
This is because neural network training solves a non-convex objective function
  that can vary greatly with the number of training vectors and features;
  therefore, it is useful only to compare performance between the two node types
  for a pair of features and number of training vectors.

We used the package default parameters to test the \texttt{pam} clustering
  benchmark function to compute the membership of feature vectors to seven
  different clusters as described in Section III.
The Sandy Bridge nodes were approximately $1.34$-$1.44$ times faster than the
  Knights Landing nodes when running the pam function for up to $17500$ feature
  vectors, but the Knights Landing nodes closed that gap to approximately $1.04$
  times as fast for $35000$ feature vectors.
With $40005$ feature vectors, the Knights Landing nodes were $1.11$ times faster
  than the Sandy Bridge nodes.

% nnet training parameters
\begin{table*}
  \caption{Training Parameters Used with Neural Network Training Benchmark}
  \label{tab:nnetParams}
  \begin{tabular}{lll}
    \toprule
    Training parameters & Description & Value\\
    \midrule
      \texttt{size}     & Number of neurons in hidden layer        & $200$ \\
      \texttt{maxit}    & Maximum number of solver iterations      & $12500$ \\
      \texttt{abstol}   & Absolute tolerance for the fit criterion & $1.0\times 10^{-6}$\\
      \texttt{reltol}   & Tolerance used to terminate solver if    & $1.0\times 10^{-11}$\\
                        & the fit criterion does not decrease by a &\\
                        & factor of at least $(1-\mathrm{\texttt{reltol}})$ &\\
      \texttt{linout}   & Enables real-valued, linear outputs      & \texttt{TRUE}\\
      \texttt{MaxNWts}  & The maximum allowable number of weights  & $4000$\\
    \bottomrule
  \end{tabular}
\end{table*}

% Neural networking benchmark results
\begin{table*}
  \caption{Run Time of Neural Network Training Benchmark With a Single Thread}
  \label{tab:nnetResults}
  \begin{tabular}{llll}
    \toprule
      Number of features & Number of training vectors & \multicolumn{2}{c}{Run time (sec)}\\
                         &                            & Sandy Bridge & Knights Landing\\
    \midrule
    $3$ & $5000$  & $3.239\times 10^{2}$ & $1.156\times 10^{3}$ \\
    $3$ & $10000$ & $1.842\times 10^{2}$ & $6.690\times 10^{2}$ \\
    $5$ & $5000$  & $3.520\times 10^{2}$ & $1.028\times 10^{3}$ \\
    $5$ & $10000$ & $1.536\times 10^{3}$ & $4.664\times 10^{3}$ \\
    $5$ & $15000$ & $2.660\times 10^{3}$ & $8.216\times 10^{3}$ \\
    \bottomrule
  \end{tabular}
\end{table*}

% Clustering benchmark results
\begin{table*}
  \caption{Run Time of Clustering Benchmark for a Single Thread}
  \label{tab:clusterResults}
  \begin{tabular}{lllll}
    \toprule
    Number of features & Number of clusters & Number of feature vectors & \multicolumn{2}{c}{Run time (sec)}\\
                       &                    &                           & Sandy Bridge & Knights Landing\\
    \midrule
    $3$ & $7$ & $7000$  & $4.416\times 10^{1}$ & $5.949\times 10^{1}$ \\
    $3$ & $7$ & $10500$ & $1.065\times 10^{2}$ & $1.536\times 10^{2}$ \\
    $3$ & $7$ & $14000$ & $2.890\times 10^{2}$ & $4.112\times 10^{2}$ \\
    $3$ & $7$ & $17500$ & $6.757\times 10^{2}$ & $9.326\times 10^{2}$ \\
    $3$ & $7$ & $35000$ & $2.578\times 10^{3}$ & $2.698\times 10^{3}$ \\
    $3$ & $7$ & $40005$ & $9.236\times 10^{3}$ & $8.313\times 10^{3}$ \\
    \bottomrule
  \end{tabular}
\end{table*}

%

The overheads in the kernel functions can potentially be large compared to
  similar functionality implemented exclusively in C due to data copying and
  validity checking that is common in interpreted languages.
The benchmark kernel functions are essentially wrapper functions, most of which
  call BLAS and LAPACK routines to perform the bulk of the
  computation~\cite{cran:Rmanuals}.
To determine the potential for further optimization of R functionality, we
  developed a set of drivers written to test the performance of a subset of the
  benchmark kernel functions in C and compared their performance to that of
  their R interpreter counterparts.
We developed C drivers for the matrix cross product, QR decomposition, and
  dense linear solve with multiple right-hand sides.
These kernel functions are each implemented as R language wrapper functions
  which call C internal functions.
The cross product internal function is implemented as a call to the LAPACK
  FORTRAN function \texttt{dsyrk} followed by a for loop to copy the upper
  triangle of the result matrix to its lower triangle.
The QR decomposition internal function is implemented as a call to the LAPACK
  FORTRAN function \texttt{dgeqp3}, and the dense linear solve internal function
  is implemented as a call to the LAPACK FORTRAN function \texttt{dgesv}.
The C driver for QR calls the \texttt{LAPACKE\_dsyrk} C wrapper function which
  dynamically allocates an optimally sized workspace, the other driver functions
  call the FORTRAN functions directly.
The R implementation of the dense linear solve computes an estimate of the
  condition number by default, a task we did not implement in the corresponding
  C driver.
The performance of the C drivers is given in Figure 13, Figure 14, and
  Figure 15.
The results show that the performance of R kernels are highly competitive with
  the native C drivers for all matrix dimensions and threads.
The performance curves for the dense linear solve applied to the smallest tested
  matrix dimensions (Figure 15) do show some performance anomalies, however.
There is a substantial difference between the strong scaling of the C driver and
  the R kernel when using two threads.
Furthermore, the C driver scalability diverges sharply from that of the R kernel
  for eight or more threads for matrix dimensions $1000$ and $2000$.
We reran the strong scaling test of the C driver several times for the smaller
  matrix dimensions to verify the results, with no appreciable change in the
  curves.

\section{Discussion}
We developed a new R HPC benchmarking package consisting of microbenchmarks of
  several computational kernels and benchmarks of machine learning functions
  from the R standard distribution.
The implementation of the benchmarks improves upon limitations of the few R
  benchmarks that have been developed to date, namely the inability to perform
  scalability studies over successively larger problem sizes and to
  independently configure how each individual benchmark is executed.
We obtained performance results running our HPC benchmark on both the Sandy
  Bridge and Knights Landing nodes of the Stampede supercomputer at the Texas
  Advanced Computing Center.
We performed the tests using the latest Intel compiler and MKL bundle available
  to users on the Sandy Bridge nodes and the Intel 17 Update 1 bundle on the
  Knights Landing nodes.
With few exceptions, the Sandy Bridge nodes achieved the best run-time
  performance for matrix dimensions up to a few thousand, an expected result
  given that the clock speed of the Sandy Bridge CPUs is roughly twice that of
  the Knights Landing CPUs.
However, the best run time achieved on the Sandy Bridge nodes was never more
  than three times as fast as the best run time achieved on the Knights Landing
  nodes for a given matrix dimension, except in the case of matrix-vector
  multiplication, for which the Sandy Bridge nodes were over six times faster
  than the Knights Landing nodes.
When we tested the Cholesky factorization, dense linear solve, matrix
  determinant, matrix-matrix multiplication, and matrix cross product kernels,
  the Knights Landing nodes achieved strong scaling speedups of up to $45$ times
  their single-threaded performance for large matrices.
Furthermore, the best run times achieved on Knights Landing nodes were up to
  $5.25$ times faster than the best run times achieved on Sandy Bridge nodes for
  the same kernels and the largest matrix dimensions.
The best run times achieved by the QR and singular value decompositions were up
  to $6.5$ times faster on Knights Landing than Sandy Bridge for the largest
  tested matrix dimensions, and the Knights Landing nodes achieved strong
  scaling speedups of up to $19$.
The Knights Landing nodes achieved modest strong scaling speedups of up to $3.5$
  for large matrix dimensions when we tested them with the eigendecomposition
  and the linear least squares fit kernels.
Due to the limited scalability of these kernels, the best run times on the
  Knights Landing nodes were only about $30$-$50\%$ smaller than the best run
  times on the Sandy Bridge nodes.
Scalability of the kernels never improved beyond $68$ threads on the Knights
  Landing nodes regardless of matrix dimension.
Tests comparing the performance of a few R kernels and equivalent kernels
  written in C showed that there was very little overhead introduced by the
  interpreter.
Given the low strong scaling achieved by the matrix-vector kernel, alternatives
  to the \texttt{dgemm} function, which R version 3.2.1 uses to implement the
  matrix-vector operation, should be explored.
Microbenchmarks of summary statistics kernels such as mean, variance, and
  covariance computation need to be added to the benchmark to improve its
  coverage of intrinsic R functionality; optimization efforts may need to be
  focused on those kernels depending on performance.
In addition to computational kernels, we also conducted performance tests of
  clustering and neural network training functions and determined that those
  functions had not been parallelized or engineered with parallelized kernel
  functions.
The Sandy Bridge nodes were substantially faster at neural network training with
  the \texttt{nnet} function for small numbers of features.
The Sandy Bridge nodes were faster at computing clusters with the \texttt{pam}
  function for $35000$ training vectors or less, but Knights Landing nodes were
  faster beyond $40000$ training vectors.
Performance testing of the neural network training and clustering functions
  should be conducted with larger numbers of features to determine if the
  performance of the Knights Landing nodes will improve with more features.
Furthermore, additional compute intensive machine learning or data analytics
  functionality commonly used in R should eventually be included in the
  benchmark to provide more extensive performance assessments.
Given the lack of parallelism and lower performance of the \textit{nnet} and
  \textit{cluster} packages on the Knights Landing nodes, future optimization
  and parallelization efforts should focus primarily on improving the
  performance of compute intensive packages.
If the performance of the \textit{nnet} and \textit{cluster} packages are an
  indication of the performance of R packages in general, most packages will
  likely need to be vectorized and either parallelized or restructured to rely
  as much as possible on parallelized computational kernels.

\begin{figure}
\includegraphics[height=2in, width=2.5in]{dual}
\caption{This is a test dual plot.}
\end{figure}

\begin{figure}
\includegraphics[height=2in, width=2.5in]{time}
\caption{This is a test timings plot.}
\end{figure}

\begin{figure}
\includegraphics[height=2in, width=2.5in]{scaling}
\caption{This is a test strong scaling plot.}
\end{figure}

\section{Future Work}
% TODO: Move future work content from Discussion section into this section

%\begin{acks}
%  The authors would like to thank...
%  See also \grantsponsor and \grantnum commands in acmguide.pdf
%
%\end{acks}
